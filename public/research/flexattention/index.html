<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="FlexAttention for Efficient High-Resolution Vision-Language Models">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>FlexAttention</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-2">FlexAttention for Efficient High-Resolution Vision-Language Models</h1>
          <!-- <h3 class="title is-5 conference-authors">ICLR 2024 Submission</h3> -->
        </div>
      </div>
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <div style="font-size: 18px;">
            <a href="https://senfu.github.io" style="color: black;">Junyan Li</a><sup>1</sup>, 
            <a href="https://chendl02.github.io" style="color: black;">Delin Chen</a><sup>1</sup>, 
            <a href="https://www.tianle.website/#/" style="color: black;">Tianle Cai</a><sup>2</sup>, 
            <a href="https://peihaochen.github.io/" style="color: black;">Peihao Chen</a><sup>3</sup>,
          </div>
          <div style="font-size: 18px;">
            <a href="https://evelinehong.github.io/" style="color: black;">Yining Hong</a><sup>4</sup>,
            <a href="https://zfchenunique.github.io" style="color: black;">Zhenfang Chen</a><sup>5</sup>
            <a href="https://scholar.google.com.hk/citations?user=qff5rRYAAAAJ" style="color: black;">Yikang Shen</a><sup>5</sup>,
            <a href="https://people.csail.mit.edu/ganchuang" style="color: black;">Chuang Gan</a><sup>1,5</sup>
          </div>
        </div>
      </div>
      <div class="columns is-centered">
        <div class="column has-text-centered" style="font-size: 14px;">
          <sup>1</sup>UMass Amherst&nbsp;&nbsp;&nbsp;&nbsp;
          <sup>2</sup>Princeton University&nbsp;&nbsp;&nbsp;&nbsp;
          <sup>3</sup>South China University of Technology
          <br>
          <sup>4</sup>University of California, Los Angeles&nbsp;&nbsp;&nbsp;&nbsp;<sup>5</sup>MIT-IBM Watson AI Lab
        </div>
      </div>
      <div class="columns is-centered">
        <a href="https://arxiv.org/abs/2407.20228" target="_blank" class="external-link button is-normal is-rounded is-dark" style="margin: 10px;">
          <span class="icon">
              <i class="ai ai-arxiv"></i>
          </span>
          <span>Paper</span>
        </a>
        <a href="https://github.com/UMass-Foundation-Model/FlexAttention" target="_blank" class="external-link button is-normal is-rounded is-dark" style="margin: 10px;">
          <span class="icon">
              <svg class="svg-inline--fa fa-github fa-w-16" aria-hidden="true" focusable="false" data-prefix="fab" data-icon="github" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512" data-fa-i2svg=""><path fill="currentColor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg><!-- <i class="fab fa-github"></i> Font Awesome fontawesome.com -->
          </span>
          <span>Code</span>
        </a>
        <a href="https://huggingface.co/senfu/llava-v1.5-7b-flexattn" target="_blank" class="external-link button is-normal is-rounded is-dark" style="margin: 10px;">
          <span class="icon">
            ðŸ¤—
          </span>
          <span>Models</span>
        </a>
      </div>
    </div>
  </div>
</section>



<section class="section" id="abstract">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <video controls autoplay loop muted playsinline>
          <source src="static/assets/teaser.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>


<div class="container is-max-desktop" id="method">
  <hr class="solid">
</div>


<section class="section" id="abstract">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Overview</h2>
        <div class="content has-text-justified">
          <p>
            Concurrent vision-language models often struggle with perceiving details in high-resolution images. Recently, some models have improved by encoding high-resolution images and using all tokens to compute attention. However, this approach significantly increases computational cost. To address this issue, we propose <i>FlexAttention</i>, a flexible attention mechanism designed for efficient high-resolution vision-language models.
          </p>
        </div>
      </div>
    </div>
  </div>

  <div class="container is-max-desktop">
    <img src="static/assets/overview.jpg" alt="overview" style="width:100%;">
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <div class="content has-text-justified" style="margin-top: 20px;">
          <p>
            The idea behind <i>FlexAttention</i> is simple: instead of using all tokens within a high-resolution image to compute attention, we propose using a subset of important high-resolution tokens, dynamically selected through the attention map. Specifically, the high-resolution input image is downsampled to produce a low-resolution version. This low-resolution image, similar to those used in other VLMs, is concatenated with text tokens and fed into the LLM. The attention map generated by the LLM's attention module is then used to select a subset of important high-resolution tokens.
          </p>
        </div>
      </div>
    </div>


    <img src="static/assets/selection.jpg" alt="selection" style="width:100%;">
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <div class="content has-text-justified" style="margin-top: 20px;">
          <p>
            This selection process occurs within the <i>Feature Selection Module</i>, where regions corresponding to the highest attention values are selected, cropped from the high-resolution feature map, and sent to the <i>Hierarchical Attention Module</i> in the next layer. The <i>Hierarchical Attention Module</i>, which replaces the original self-attention module, computes the attention between the selected high-resolution tokens, low-resolution tokens, and text tokens. This allows the model to focus on important regions in the high-resolution image and efficiently obtain detailed information.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>


<div class="container is-max-desktop" id="method">
  <hr class="solid">
</div>



<section class="section" id="visualization">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Attention Map Visualization</h2>
      </div>
    </div>
    <div class="columns is-centered has-text-centered">
        <img src="static/assets/0.gif" alt="visualization" style="width:80%;">
    </div>
    <div class="columns is-centered">
      <p>
        Question: What is the brand of this camera?
        <br>
        Answer: Dakota digital
      </p>
    </div>
    <hr class="solid">
    <div class="columns is-centered has-text-centered">
      <img src="static/assets/40.gif" alt="visualization" style="width:80%;">
    </div>
    <div class="columns is-centered">
      <p>
        Question: What is the number on the runner in middle?
        <br>
        Answer: 57859
      </p>
    </div>
    <hr class="solid">
    <div class="columns is-centered has-text-centered">
      <img src="static/assets/69.gif" alt="visualization" style="width:80%;">
    </div>
    <div class="columns is-centered">
      <p>
        Question: Who wrote this book?
        <br>
        Answer: Ray Kurzweil
      </p>
    </div>
  </div>

</section>

<div class="container is-max-desktop" id="method">
  <hr class="solid">
</div>

<!-- Citation -->
<section class="section" id="citation">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Citation</h2>
        <div class="content has-text-justified">
          <pre class="p-4 mb-5 bg-light rounded" style="text-wrap: wrap; color: #555;">
@misc{li2024flexattention,
      title={FlexAttention for Efficient High-Resolution Vision-Language Models}, 
      author={Junyan Li and Delin Chen and Tianle Cai and Peihao Chen and Yining Hong and Zhenfang Chen and Yikang Shen and Chuang Gan},
      year={2024},
      eprint={2407.20228},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2407.20228}, 
}</pre>
        </div>
      </div>
    </div>
  </div>



<!-- Default Statcounter code for flexattention
https://flexattention.github.io/ -->
<script type="text/javascript">
  var sc_project=12978195; 
  var sc_invisible=1; 
  var sc_security="41250ae2"; 
  </script>
  <script type="text/javascript"
  src="https://www.statcounter.com/counter/counter.js"
  async></script>
  <noscript><div class="statcounter"><a title="web counter"
  href="https://statcounter.com/" target="_blank"><img
  class="statcounter"
  src="https://c.statcounter.com/12978195/0/41250ae2/1/"
  alt="web counter"
  referrerPolicy="no-referrer-when-downgrade"></a></div></noscript>
<!-- End of Statcounter Code -->

</body>
</html>
