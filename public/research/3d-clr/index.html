
<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<script src="bootstrap.js"></script>
<script type="text/javascript" charset="utf-8" src="https://ajax.googleapis.com/ajax/libs/jquery/1.3.2/jquery.min.js"></script> 
<!---
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
--->
<script src="load-mathjax.js" async></script>
<script>
    function setImageGLIDE(select){
       var image = document.getElementsByName("image-swap-1")[0];
       image.src = select.options[select.selectedIndex].value;
    }
    function setImageStable(select){
       var image = document.getElementsByName("image-swap-2")[0];
       image.src = select.options[select.selectedIndex].value;
    }
</script>

<style type="text/css">
body {
    font-family: "Titillium Web", "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
    font-weight: 300;
    font-size: 17px;
    margin-left: auto;
    margin-right: auto;
}

@media screen and (min-width: 980px){
    body {
        width: 980px;
    }
}


h1 {
    font-weight:300;
    line-height: 1.15em;
}

h2 {
    font-size: 1.75em;
}
a:link,a:visited {
    color: #5364cc;
    text-decoration: none;
}
a:hover {
    color: #208799;
}
h1 {
    text-align: center;
}
h2,h3 {
    text-align: left;
}

h1 {
    font-size: 40px;
    font-weight: 500;
}
h2 {
    font-weight: 400;
    margin: 16px 0px 4px 0px;
}
h3 {
    font-weight: 600;
    margin: 16px 0px 4px 0px;
}

.paper-title {
    padding: 1px 0px 1px 0px;
}
section {
    margin: 32px 0px 32px 0px;
    text-align: justify;
    clear: both;
}
.col-5 {
     width: 20%;
     float: left;
}

.move-down {
    margin-top:1.2cm;
}

.col-4 {
     width: 25%;
     float: left;
}
.col-3 {
     width: 33%;
     float: left;
}
.col-2 {
     width: 50%;
     float: left;
}
.col-1 {
     width: 100%;
     float: left;
}

.author-row, .affil-row {
    font-size: 26px;
}

.author-row-new { 
    text-align: center; 
}

.author-row-new a {
    display: inline-block;
    font-size: 20px;
    padding: 4px;
}

.author-row-new sup {
    color: #313436;
    font-size: 12px;
}

.affiliations-new {
    font-size: 18px;
    text-align: center;
    width: 80%;
    margin: 0 auto;
    margin-bottom: 20px;
}

.row {
    margin: 16px 0px 16px 0px;
}
.authors {
    font-size: 26px;
}
.affiliatons {
    font-size: 18px;
}
.affil-row {
    margin-top: 18px;
}
.teaser {
    max-width: 100%;
}
.text-center {
    text-align: center;  
}
.screenshot {
    width: 256px;
    border: 1px solid #ddd;
}
.screenshot-el {
    margin-bottom: 16px;
}
hr {
    height: 1px;
    border: 0; 
    border-top: 1px solid #ddd;
    margin: 0;
}
.material-icons {
    vertical-align: -6px;
}
p {
    line-height: 1.25em;
}
.caption {
    font-size: 16px;
    color: #666;
    margin-top: 4px;
    margin-bottom: 10px;
	text-align: left;
}


video {
    display: block;
    margin: auto;
}


figure {
    display: block;
    margin: auto;
    margin-top: 10px;
    margin-bottom: 10px;
}
#bibtex pre {
    font-size: 14px;
    background-color: #eee;
    padding: 16px;
}
.blue {
    color: #2c82c9;
    font-weight: bold;
}
.orange {
    color: #d35400;
    font-weight: bold;
}
.flex-row {
    display: flex;
    flex-flow: row wrap;
    padding: 0;
    margin: 0;
    list-style: none;
}

.paper-btn-coming-soon {
    position: relative; 
    top: 0;
    left: 0;
}

.coming-soon {
    position: absolute;
    top: -15px;
    right: -15px;
}

.center {
  margin-left: 10.0%;
  margin-right: 10.0%;
}

.paper-btn {
  position: relative;
  text-align: center;

  display: inline-block;
  margin: 8px;
  padding: 8px 8px;

  border-width: 0;
  outline: none;
  border-radius: 2px;
  
  background-color: #5364cc;
  color: white !important;
  font-size: 20px;
  width: 100px;
  font-weight: 600;
}
.paper-btn-parent {
    display: flex;
    justify-content: center;
    margin: 16px 0px;
}

.paper-btn:hover {
    opacity: 0.85;
}

.container {
    margin-left: auto;
    margin-right: auto;
    padding-left: 16px;
    padding-right: 16px;
}

.venue {
    font-size: 23px;
}

.topnav {
    background-color: #EEEEEE;
    overflow: hidden;
}

.topnav div {
    max-width: 1070px;
    margin: 0 auto;
}

.topnav a {
    display: inline-block;
    color: black;
    text-align: center;
    vertical-align: middle;
    padding: 16px 16px;
    text-decoration: none;
    font-size: 18px;
}

.topnav img {
    padding: 2px 0px;
    width: 100%;
    margin: 0.2em 0px 0.3em 0px;
    vertical-align: middle;
}

pre {
    font-size: 0.9em;
    padding-left: 7px;
    padding-right: 7px;
    padding-top: 3px;
    padding-bottom: 3px;
    border-radius: 3px;
    background-color: rgb(235, 235, 235);
    overflow-x: auto;
}

.download-thumb {
    display: flex;
}

@media only screen and (max-width: 620px) {
    .download-thumb {
        display: none;
    }
}

.paper-stuff {
    width: 50%;
    font-size: 20px;
}

@media only screen and (max-width: 620px) {
    .paper-stuff {
        width: 100%;
    }
}
* {
  box-sizing: border-box;
}

.column {
  text-align: center;
  float: left;
  width: 16.666%;
  padding: 5px;
}
.column3 {
  text-align: center;
  float: left;
  width: 33.333%;
  padding: 5px;
}
.column4 {
  text-align: center;
  float: left;
  width: 50%;
  padding: 5px;
}
.column5 {
  text-align: center;
  float: left;
  width: 20%;
  padding: 5px;
}
.column10 {
  text-align: center;
  float: left;
  width: 10%;
  padding: 5px;
}
.border-right {
    border-right: 1px solid black;
}
.border-bottom{
    border-bottom: 1px solid black;
}


.row-center {
    margin: 16px 0px 16px 0px;
    text-align: center;
}

/* Clearfix (clear floats) */
.row::after {
  content: "";
  clear: both;
  display: table;
}
.img-fluid {
  max-width: 100%;
  height: auto;
}
.figure-img {
  margin-bottom: 0.5rem;
  line-height: 1;
}








.rounded-circle {
  border-radius: 50% !important;
}






/* Responsive layout - makes the three columns stack on top of each other instead of next to each other */
@media screen and (max-width: 500px) {
  .column {
    width: 100%;
  }
}
@media screen and (max-width: 500px) {
  .column3 {
    width: 100%;
  }
}

</style>
<link rel="stylesheet" href="bootstrap-grid.css">

<script type="text/javascript" src="../js/hidebib.js"></script>
    <link href='https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic' rel='stylesheet' type='text/css'>
    <head>
        <title> 3D Concept Learning and Reasoning from Multi-View Images</title>
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta property="og:description" content="3D Concept Learning and Reasoning from Multi-View Images"/>
        <link href="https://fonts.googleapis.com/css2?family=Material+Icons" rel="stylesheet">
        <meta name="twitter:card" content="summary_large_image">
        <meta name="twitter:creator" content="@yining_hong">
        <meta name="twitter:title" content="3D Concept Learning and Reasoning from Multi-View Images">
        <meta name="twitter:description" content="">
        <meta name="twitter:image" content="">
    </head>

 <body>


<div class="container">
    <div class="paper-title">
    <h1> 
        3D Concept Learning and Reasoning from Multi-View Images
    </div>

    <div id="authors">
        <center>
            <div class="author-row-new">
                <a href="https://evelinehong.github.io/"> Yining Hong<sup>1</sup></a>,
                <a href="https://xhrlyb.github.io/"> Chunru Lin<sup>2</sup></a>,
                <a href="https://yilundu.github.io/"> Yilun Du<sup>3</sup></a>,
                <a href="https://zfchenunique.github.io/"> Zhenfang Chen<sup>4</sup></a>,
                <a href="http://web.mit.edu/cocosci/josh.html"> Josh Tenenbaum<sup>3</sup></a>
                <a href="https://people.csail.mit.edu/ganchuang/"> Chuang Gan<sup>4,5</sup></a>,
            </div>
        </center>
        <center>
        <div class="affiliations">
            <span><sup>1</sup> UCLA</span>
            <span><sup>2</sup> Shanghai Jiao Tong University</span>
            <span><sup>3</sup> MIT</span>
            <span><sup>4</sup> MIT-IBM Watson AI Lab</span>
            <span><sup>5</sup> Umass Amherst</span><br/>
        </div>

        <div class="affil-row">
            <div class="venue text-center"><b>CVPR 2023 </b></div>
        </div>

        </center>

        <div style="clear: both">
            <div class="paper-btn-parent">
            <a class="paper-btn" href="https://arxiv.org/abs/2303.11327">
                <span class="material-icons"> description </span> 
                 Paper
            </a>
            <a class="paper-btn" href="https://github.com/evelinehong/3D-CLR-Official">
                <span class="material-icons"> code </span>
                Code & Data
            </a>
            </div>
        </div>
    </div>


    <section id="teaser-image">
        <center>
        <span><strong>Question: </strong></span>
        <label for="composed_stable_diffusion">
            <select id="composed_stable_diffusion" class="dropselect" style="text-align: center" name="composed_stale_diffusion_generation" onchange="setImageStable(this);">
                <option value="materials/q1.mp4" selected="">Is there a table with television on top of it?</option>
                <option value="materials/questions2.mp4">Are there more windows than lights in the room?</option>
                <option value="materials/question3.mp4">Are there any curtains in the room?</option>
                <option value="materials/feature1.mp4">Visualization of 3D features and concept grounding on the neural field</option>
            </select>
        </label>
        </center>

        <center>
            <figure>
            <video src="materials/q1.mp4" class="centered" width="55%" autoplay loop muted playsinline class="video-background" name="image-swap-2">
            </figure>

        </center>

        <!-- <center>
            <figure>
                <video class="centered" width="50%" autoplay loop muted playsinline class="video-background " >
                    <source src="materials/question1.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                </video>
            </figure>

        </center> -->
    </section>

    
    <section id="abstract"/>
        <hr>
        <h2>Abstract</h2>
        <div class="flex-row">
            <p>
                Humans are able to accurately reason in 3D by gathering multi-view observations of the surrounding world.  Inspired by this insight, we introduce a new large-scale benchmark for 3D multi-view visual question answering (3DMV-VQA). This dataset is collected by an embodied agent actively moving and capturing RGB images in an environment using the Habitat simulator.  In total, it consists of approximately 5k scenes, 600k images, paired with 50k questions. We evaluate various state-of-the-art models for visual reasoning on our benchmark and find that they all perform poorly. We suggest that a principled approach for 3D reasoning from multi-view images should be to infer a compact 3D representation of the world from the multi-view images, which is further grounded on open-vocabulary semantic concepts, and then to execute reasoning on these 3D representations. As the first step towards this approach, we propose a novel 3D concept learning and reasoning (3D-CLR) framework that seamlessly combines these components via neural fields, 2D pre-trained vision-language models, and neural reasoning operators. Experimental results suggest that our framework outperforms baseline models by a large margin, but the challenge remains largely unsolved. We further perform an in-depth analysis of the challenges and highlight potential future directions.
            </p>
        </div>
    </section>
    <section id="dataset"/>
        <hr>
        <h2>3DMV-VQA Dataset</h2>
        <div class="flex-row">
            <p>
                Humans are able to accurately reason in 3D by gathering
  multi-view observations of the surrounding world. Inspired
  by this insight, we introduce a new large-scale benchmark
  for 3D multi-view visual question answering (3DMV-VQA).
  This dataset is collected by an embodied agent actively mov-
  ing and capturing RGB images in an environment using the
  Habitat simulator. Here we show an exemplar scene with multi-view images and question-answer pairs of our 3DMV-VQA dataset. 3DMV-VQA contains four question types: concept, counting, relation, comparison. Orange words denote semantic concepts; blue words denote the relations.
            </p>
        </div>
        <figure>
            <a>
                <img width="15%" src="materials/1.gif"> 
            </a>
            <a>
                <img width="15%" src="materials/2.gif"> 
            </a>
            <a>
                <img width="15%" src="materials/3.gif"> 
            </a>
            <a>
                <img width="15%" src="materials/6.gif"> 
            </a>
            <a>
                <img width="15%" src="materials/7.gif"> 
            </a>
            <a>
                <img width="15%" src="materials/8.gif"> 
            </a>
            <p class="caption">
                A robot agent explores the environment to collect multi-view images for 3D reasoning.
            </p> 
        </figure>
        <figure>
            <a>
                <img width="100%" src="materials/dataset.png"> 
            </a>
            <p class="caption">
                An exemplar scene with multi-view images and question-answer pairs of our 3DMV-VQA dataset. 3DMV-VQA contains four
question types: concept, counting, relation, comparison. Orange words denote semantic concepts; blue words denote the relations.
            </p> <br>
        </figure>

    </section>
    <section id="method"/>
        <hr>
        <h2>3D-CLR Framework</h2>
        <figure>
            <a>
                <img width="100%" src="materials/framework.png"> 
            </a>

        </figure>

        <div class="flex-row">
            <p> 
                An overview of our 3D-CLR framework. First, we learn a 3D compact scene representation from multi-view images using neural
                fields (I). Second, we use CLIP-LSeg model to get per-pixel 2D features (II). We utilize a 3D-2D alignment loss to assign features to the 3D
                compact representation (III). By calculating the dot-product attention between the 3D per-point features and CLIP language embeddings, we
                could get the concept grounding in 3D (IV). Finally, the reasoning process is performed via a set of neural reasoning operators, such as
                FILTER, GET INSTANCE and COUNT RELATION (V). Relation operators are learned via relation networks.
            </p>
        </div>
    </section>




        

    <section id="results">
        <hr>
        <h2>3D Concept Grounding on the Neural Fields</h2>  

            <figure>
                <a>
                    <img width="40%" src="materials/4.gif" style="padding-left: 30px"> 
                </a>
                <a>
                    <img width="40%" src="materials/5.gif"> 
                </a>
                <p class="caption">
                    By calculating attention between CLIP language features and learned 3D features, we can ground concepts on the neural field.
                </p> <br>
            </figure>
            </center>

        <!-- <hr> -->


        <h2>Results on HM3D Dataset</h2>  

            <center>
            <figure>
                <a>
                    <img width="100%" src="materials/visualization.png"> 
                </a>
                <p class="caption">
                    Qualitative examples of our 3D-CLR. We can see that 3D-CLR can ground most of the concepts and answer most questions
correctly. However, it still fails sometimes, mainly because it cannot separate close object instances and ground small objects.
                </p> <br>
            </figure>
            </center>

        <!-- <hr> -->

        <h2>Generalization to Replica Dataset</h2>  
            <center>
            <figure>
                <a>
                    <img width="100%" src="materials/replica.png"> 
                </a>
                <p class="caption">
                    To further show that 3D-CLR trained on HM3D can be generalized to new reasoning datasets, we further
collect a small visual question answering dataset on Replica with Habitat following the same data generation as HM3D. We can see that 3D-CLR can maintain the
performance on Replica as it performs on HM3D, which shows 3D-CLR’s good generalization ability
                </p> <br>
            </figure>
            </center>

        <hr>

    </section> 




    <section id="related_projects">
        <hr>
        <h2>Related Projects</h2>  

        <div class="row vspace-top">
        <div class="col-sm-5">
            <img width="100%" src="materials/3d-cg2.png" style="padding-top: 40px"> 
            <img width="100%" src="materials/3d-cg.png" style="padding-top: 40px"> 
        </div>
        <div class="col-sm-7">
          <div class="paper-title">
            <a href="https://arxiv.org/abs/2207.06403">3D Concept Grounding on Neural Fields</a>
        </div>
        <div>
            We propose to leverage the continuous, differentiable nature of neural fields to segment and learn concepts. Specifically, each 3D coordinate in a scene is represented as a high-dimensional descriptor. Concept grounding can then be performed by computing the similarity between the descriptor vector of a 3D coordinate and the vector embedding of a language concept, which enables segmentations and concept learning to be jointly learned on neural fields in a differentiable fashion. As a result, both 3D semantic and instance segmentations can emerge directly from question answering supervision using a set of defined neural operators on top of neural fields (e.g., filtering and counting). Experimental results show that our proposed framework outperforms unsupervised/language-mediated segmentation models on semantic and instance segmentation tasks, as well as outperforms existing models on the challenging 3D aware visual reasoning tasks. Furthermore, our framework can generalize well to unseen shape categories and real scans. 
        </div>
        </div>
        </div>

    </section> 

    <section id="bibtex">
        <h2>Citation</h2>
        <hr>
        <pre><code>@article{hong2023threedclr,
            title={3D Concept Learning and Reasoning from Multi-View Images},
            author={Hong, Yining and Lin, Chunru and Du, Yilun and Chen, Zhenfang and Tenenbaum, Joshua B and Gan, Chuang},
            journal={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
            year={2023}
}</code></pre>
</section>




    

</div>
</body>
</html>
